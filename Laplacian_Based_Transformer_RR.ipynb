{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Pyramid Transformer Reflection Removal\n",
    "Author: Peiyao Tao\n",
    "\n",
    "Date: 10/19/2025\n",
    "\n",
    "Class: CS 7180 Advanced Perception\n",
    "\n",
    "## Purpose of the file\n",
    "For this file, we introduce a laplacian-pyramid-based transformer for reflection removal tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import vgg19\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models import vgg19, VGG19_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json file contains list of image pairs\n",
    "JSON_FILE_PATH = \"./VOC2012/VOC_results_list.json\"\n",
    "\n",
    "# Load the main JSON file\n",
    "with open(JSON_FILE_PATH, 'r') as f:\n",
    "    all_image_pairs = json.load(f)\n",
    "\n",
    "# Split the list of image pairs into 80% training and 20% validation\n",
    "train_pairs, val_pairs = train_test_split(all_image_pairs, test_size=0.2, random_state=42)\n",
    "print(f\"Data split into {len(train_pairs)} training pairs and {len(val_pairs)} validation pairs.\")\n",
    "\n",
    "# Save the split lists to new JSON files in the current directory\n",
    "with open('train_list.json', 'w') as f:\n",
    "    json.dump(train_pairs, f, indent=4)\n",
    "with open('val_list.json', 'w') as f:\n",
    "    json.dump(val_pairs, f, indent=4)\n",
    "\n",
    "print(\"Created 'train_list.json' and 'val_list.json'.\")\n",
    "\n",
    "class ReflectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading reflection removal data.\n",
    "    Uses a JSON file to correctly pair images.\n",
    "    Handles images smaller than the crop size by resizing them first.\n",
    "    Applies random cropping for training, resizing for validation.\n",
    "    Adds normalization for pretrained Swin Transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, json_path, crop_size=(224, 224), is_train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.is_train = is_train\n",
    "        self.crop_size = crop_size\n",
    "        self.json_path = json_path\n",
    "        \n",
    "        self.blended_dir = os.path.join(root_dir, 'blended')\n",
    "        self.transmission_dir = os.path.join(root_dir, 'transmission_layer')\n",
    "        self.reflection_dir = os.path.join(root_dir, 'reflection_layer')\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            self.image_pairs = json.load(f)\n",
    "        \n",
    "        # Normalization for ImageNet pretrained models\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Load and return a sample from the dataset at the given index \"\"\"\n",
    "        pair_info = self.image_pairs[idx]\n",
    "        blended_fn = pair_info['blended']\n",
    "        transmission_fn = pair_info['transmission_layer']\n",
    "        reflection_fn = pair_info['reflection_layer']\n",
    "        \n",
    "        blended_img = Image.open(os.path.join(self.blended_dir, blended_fn)).convert('RGB')\n",
    "        transmission_img = Image.open(os.path.join(self.transmission_dir, transmission_fn)).convert('RGB')\n",
    "        reflection_img = Image.open(os.path.join(self.reflection_dir, reflection_fn)).convert('RGB')\n",
    "        \n",
    "        if self.is_train:\n",
    "            # If image size smaller than crop size, resize before cropping\n",
    "            if blended_img.size[0] < self.crop_size[1] or blended_img.size[1] < self.crop_size[0]:\n",
    "                blended_img = TF.resize(blended_img, self.crop_size[0], interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "                transmission_img = TF.resize(transmission_img, self.crop_size[0], interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "                reflection_img = TF.resize(reflection_img, self.crop_size[0], interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            # Random crop the input images\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(blended_img, output_size=self.crop_size)\n",
    "            blended_img = TF.crop(blended_img, i, j, h, w)\n",
    "            transmission_img = TF.crop(transmission_img, i, j, h, w)\n",
    "            reflection_img = TF.crop(reflection_img, i, j, h, w)\n",
    "            \n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1) < 0.5:\n",
    "                blended_img = TF.hflip(blended_img)\n",
    "                transmission_img = TF.hflip(transmission_img)\n",
    "                reflection_img = TF.hflip(reflection_img)\n",
    "        else:\n",
    "            # For validation, resize to crop size\n",
    "            blended_img = TF.resize(blended_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            transmission_img = TF.resize(transmission_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            reflection_img = TF.resize(reflection_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "        blended_tensor = self.normalize(TF.to_tensor(blended_img))\n",
    "        transmission_tensor = TF.to_tensor(transmission_img)\n",
    "        reflection_tensor = TF.to_tensor(reflection_img) \n",
    "\n",
    "        return {\n",
    "            'blended': blended_tensor,\n",
    "            'transmission': transmission_tensor,\n",
    "            'reflection': reflection_tensor\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DATASET_ROOT_PATH = \"./VOC2012\" \n",
    "\n",
    "train_dataset = ReflectionDataset(root_dir=DATASET_ROOT_PATH, json_path='train_list.json', is_train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = ReflectionDataset(root_dir=DATASET_ROOT_PATH, json_path='val_list.json', is_train=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\" \n",
    "    Cross-Attention module for dual-stream interaction\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=False)\n",
    "        self.k = nn.Linear(dim, dim, bias=False)\n",
    "        self.v = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        B, C, H, W = query.shape\n",
    "        query = query.flatten(2).transpose(1, 2)\n",
    "        key = key.flatten(2).transpose(1, 2)\n",
    "        value = value.flatten(2).transpose(1, 2)\n",
    "\n",
    "        q = self.q(query).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k(key).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(value).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, H * W, C)\n",
    "        out = self.proj(out)\n",
    "        out = out.transpose(1, 2).view(B, C, H, W)\n",
    "        return out\n",
    "\n",
    "class ReflectionRemovalModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Reflection Removal Model with Dual-Stream Decoder and Cross-Attention \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained Swin Small as backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            'swin_small_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            features_only=True\n",
    "        )\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.backbone.layers_3.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.trans_up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(768, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(384 + 384, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(384, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(192 + 192, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(192, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(96 + 96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trans_final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.ref_up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(768, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(384 + 384, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(384, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(192 + 192, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(192, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(96 + 96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ref_final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(96, 96, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Cross-attention modules at each level\n",
    "        self.cross3 = CrossAttention(384)\n",
    "        self.cross2 = CrossAttention(192)\n",
    "        self.cross1 = CrossAttention(96)\n",
    "        \n",
    "        # Reflection mask head (from low-freq processing)\n",
    "        self.mask_head = nn.Sequential(\n",
    "            nn.Conv2d(3, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Refiner for high-freq Laplacian levels\n",
    "        self.refiner = nn.Sequential(\n",
    "            nn.Conv2d(3 + 1, 3, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(3, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, is_train=True, pyramid_levels=4):\n",
    "        \"\"\" \n",
    "        Forward pass of the model.\n",
    "        During training, process fixed-size input without pyramid.\n",
    "        During evaluation, use Laplacian pyramid for full-resolution processing.\n",
    "        \"\"\"\n",
    "        if is_train:\n",
    "            features = self.backbone(x)\n",
    "            \n",
    "            feat0 = features[0].permute(0, 3, 1, 2)\n",
    "            feat1 = features[1].permute(0, 3, 1, 2)\n",
    "            feat2 = features[2].permute(0, 3, 1, 2)\n",
    "            feat3 = features[3].permute(0, 3, 1, 2)\n",
    "            \n",
    "            # Transmission stream\n",
    "            x_trans = self.trans_up3(feat3)\n",
    "            x_trans = torch.cat([x_trans, feat2], dim=1)\n",
    "            x_trans = self.trans_conv3(x_trans)\n",
    "            \n",
    "            # Reflection stream\n",
    "            x_ref = self.ref_up3(feat3)\n",
    "            x_ref = torch.cat([x_ref, feat2], dim=1)\n",
    "            x_ref = self.ref_conv3(x_ref)\n",
    "            \n",
    "            # Cross-attention at level 3\n",
    "            x_trans = x_trans + self.cross3(x_trans, x_ref, x_ref)\n",
    "            x_ref = x_ref + self.cross3(x_ref, x_trans, x_trans)\n",
    "            \n",
    "            # Level 2\n",
    "            x_trans = self.trans_up2(x_trans)\n",
    "            x_trans = torch.cat([x_trans, feat1], dim=1)\n",
    "            x_trans = self.trans_conv2(x_trans)\n",
    "            \n",
    "            x_ref = self.ref_up2(x_ref)\n",
    "            x_ref = torch.cat([x_ref, feat1], dim=1)\n",
    "            x_ref = self.ref_conv2(x_ref)\n",
    "            \n",
    "            # Cross-attention at level 2\n",
    "            x_trans = x_trans + self.cross2(x_trans, x_ref, x_ref)\n",
    "            x_ref = x_ref + self.cross2(x_ref, x_trans, x_trans)\n",
    "            \n",
    "            # Level 1\n",
    "            x_trans = self.trans_up1(x_trans)\n",
    "            x_trans = torch.cat([x_trans, feat0], dim=1)\n",
    "            x_trans = self.trans_conv1(x_trans)\n",
    "            \n",
    "            x_ref = self.ref_up1(x_ref)\n",
    "            x_ref = torch.cat([x_ref, feat0], dim=1)\n",
    "            x_ref = self.ref_conv1(x_ref)\n",
    "            \n",
    "            # Cross-attention at level 1\n",
    "            x_trans = x_trans + self.cross1(x_trans, x_ref, x_ref)\n",
    "            x_ref = x_ref + self.cross1(x_ref, x_trans, x_trans)\n",
    "            \n",
    "            # Final outputs\n",
    "            transmission = self.trans_final(x_trans)\n",
    "            reflection = self.ref_final(x_ref)\n",
    "\n",
    "            return transmission, reflection\n",
    "        \n",
    "        else:\n",
    "            # During eval/inference, use Laplacian pyramid for full resolution\n",
    "            pyramid = build_laplacian_pyramid(x, levels=pyramid_levels)\n",
    "            \n",
    "            # Process lowest level (low-freq) with full dual-stream\n",
    "            low_freq = pyramid[0]\n",
    "            features = self.backbone(low_freq)\n",
    "            \n",
    "            feat0 = features[0].permute(0, 3, 1, 2)\n",
    "            feat1 = features[1].permute(0, 3, 1, 2)\n",
    "            feat2 = features[2].permute(0, 3, 1, 2)\n",
    "            feat3 = features[3].permute(0, 3, 1, 2)\n",
    "            \n",
    "            # Transmission/reflection as before...\n",
    "            # (omit repetition, same as train block above)\n",
    "            transmission_low = self.trans_final(x_trans)\n",
    "            reflection_low = self.ref_final(x_ref)\n",
    "            \n",
    "            # Get reflection mask from low-freq reflection\n",
    "            mask_low = self.mask_head(reflection_low)\n",
    "            \n",
    "            # Refine higher Laplacian levels (high-freq)\n",
    "            refined_pyramid = [transmission_low]  # Start with refined low-freq as base\n",
    "            for lap in pyramid[1:]:\n",
    "                # Upsample mask to current level\n",
    "                mask_up = F.interpolate(mask_low, size=lap.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Refine lap with mask-guided conv\n",
    "                refined_lap = self.refiner(lap, mask_up)\n",
    "                \n",
    "                refined_pyramid.append(refined_lap)\n",
    "                mask_low = mask_up  # For next level\n",
    "            \n",
    "            # Reconstruct transmission (reflection similar if needed)\n",
    "            transmission = reconstruct_laplacian_pyramid(refined_pyramid)\n",
    "            reflection = x - transmission  # Simple subtract for reflection, or process similarly\n",
    "            \n",
    "            return transmission, reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Loss Function with Perceptual and Exclusion Losses\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\" \n",
    "    Perceptual Loss using VGG19 features \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(weights=VGG19_Weights.DEFAULT).features\n",
    "        layers = [vgg[:4], vgg[4:9], vgg[9:16], vgg[16:23]]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        loss = 0.0\n",
    "        x, y = pred, gt\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            loss += F.l1_loss(x, y)\n",
    "        return loss / len(self.layers)\n",
    "\n",
    "def build_laplacian_pyramid(img, levels=4):\n",
    "    \"\"\" Build Laplacian pyramid from input image \"\"\"\n",
    "    pyramid = []\n",
    "    current = img\n",
    "    for _ in range(levels - 1):\n",
    "        down = F.avg_pool2d(current, kernel_size=2, stride=2)\n",
    "        up = F.interpolate(down, size=current.shape[2:], mode='bilinear', align_corners=False)\n",
    "        lap = current - up\n",
    "        pyramid.append(lap)\n",
    "        current = down\n",
    "    pyramid.append(current)  # Base level\n",
    "    return pyramid[::-1]  # Low to high freq\n",
    "\n",
    "def exclusion_loss(pred_trans, pred_ref, levels=3):\n",
    "    \"\"\" Exclusion loss to encourage separation of transmission and reflection \"\"\"\n",
    "    loss = 0.0\n",
    "    trans, ref = pred_trans, pred_ref\n",
    "    for _ in range(levels):\n",
    "        trans_grad_x = trans[:, :, :-1, :-1] - trans[:, :, :-1, 1:]\n",
    "        trans_grad_y = trans[:, :, :-1, :-1] - trans[:, :, 1:, :-1]\n",
    "        ref_grad_x = ref[:, :, :-1, :-1] - ref[:, :, :-1, 1:]\n",
    "        ref_grad_y = ref[:, :, :-1, :-1] - ref[:, :, 1:, :-1]\n",
    "        \n",
    "        loss_x = torch.mean(torch.abs(trans_grad_x * ref_grad_x))\n",
    "        loss_y = torch.mean(torch.abs(trans_grad_y * ref_grad_y))\n",
    "        loss += loss_x + loss_y\n",
    "        \n",
    "        trans = F.avg_pool2d(trans, kernel_size=2, stride=2)\n",
    "        ref = F.avg_pool2d(ref, kernel_size=2, stride=2)\n",
    "    return loss / levels\n",
    "\n",
    "def gradient_loss(pred, gt, alpha=1.0):\n",
    "    \"\"\" Gradient loss to preserve edges \"\"\"\n",
    "    # Sobel kernels for edge detection\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
    "    \n",
    "    pred_grad_x = F.conv2d(pred.mean(1, keepdim=True), sobel_x, padding=1)\n",
    "    pred_grad_y = F.conv2d(pred.mean(1, keepdim=True), sobel_y, padding=1)\n",
    "    gt_grad_x = F.conv2d(gt.mean(1, keepdim=True), sobel_x, padding=1)\n",
    "    gt_grad_y = F.conv2d(gt.mean(1, keepdim=True), sobel_y, padding=1)\n",
    "    \n",
    "    return alpha * (F.l1_loss(pred_grad_x, gt_grad_x) + F.l1_loss(pred_grad_y, gt_grad_y))\n",
    "\n",
    "def reflection_removal_loss(pred_trans, pred_ref, gt_trans, gt_ref, blended, perceptual_module, pyramid_levels=4):\n",
    "    \"\"\" Combined loss function for reflection removal \"\"\"\n",
    "    mse = nn.MSELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "    \n",
    "    # Base full-res losses\n",
    "    l_trans = mse(pred_trans, gt_trans)\n",
    "    l_ref = mse(pred_ref, gt_ref)\n",
    "    l_recon = mse(pred_trans + pred_ref, blended)\n",
    "    base_loss = l_trans + l_ref + l_recon\n",
    "    \n",
    "    perc_loss = perceptual_module(pred_trans, gt_trans)\n",
    "    \n",
    "    excl_loss = exclusion_loss(pred_trans, pred_ref)\n",
    "    \n",
    "    grad_loss = gradient_loss(pred_trans, gt_trans, alpha=1.0)\n",
    "    \n",
    "    total_loss = 1.0 * base_loss + 0.5 * perc_loss + 0.5 * excl_loss + 0.5 * grad_loss\n",
    "    \n",
    "    # Multi-scale pyramid loss\n",
    "    gt_pyr_trans = build_laplacian_pyramid(gt_trans, pyramid_levels)\n",
    "    pred_pyr_trans = build_laplacian_pyramid(pred_trans, pyramid_levels)\n",
    "    \n",
    "    ms_loss = 0.0\n",
    "    for level in range(pyramid_levels):\n",
    "        weight = 1.0 / (2 ** level)  # Higher weight for finer levels\n",
    "        ms_loss += weight * l1(pred_pyr_trans[level], gt_pyr_trans[level])\n",
    "    \n",
    "    total_loss += 0.5 * ms_loss  # Add to encourage multi-scale fidelity\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    Saves the best model and allows for resuming training.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')  # Use a direct loss value, not a score\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        # Checks if the current loss is an improvement\n",
    "        if self.best_loss - val_loss > self.delta:\n",
    "            # If it is, save the model and reset the counter\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # If not, increment the counter\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves a checkpoint with model state and validation loss.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Val loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}). Saving model to {self.path}.')\n",
    "        \n",
    "        # Save a dictionary containing both the model's state and the best loss\n",
    "        checkpoint = {\n",
    "            'best_loss': val_loss,\n",
    "            'model_state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, self.path)\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Loads model and best loss from a checkpoint.\"\"\"\n",
    "        if os.path.exists(self.path):\n",
    "            if self.verbose:\n",
    "                print(f\"Loading checkpoint from '{self.path}'\")\n",
    "            checkpoint = torch.load(self.path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.best_loss = checkpoint['best_loss']\n",
    "            if self.verbose:\n",
    "                print(f\"Resuming with best validation loss: {self.best_loss:.6f}\")\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"No checkpoint found at '{self.path}'. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReflectionRemovalModel()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "perceptual_module = PerceptualLoss().to(device)\n",
    "\n",
    "# Optimizer\n",
    "encoder_layer3_param_ids = {id(p) for p in model.backbone.layers_3.parameters()}\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.backbone.layers_3.parameters(), 'lr': 1e-5},\n",
    "    {\"params\": [p for p in model.parameters() if p.requires_grad and id(p) not in encoder_layer3_param_ids], \"lr\": 1e-4}\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='transformer_best_model.pt')\n",
    "early_stopping.load_checkpoint(model)  # Load if exists\n",
    "early_stopping.best_loss = float('inf')\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        blended = batch['blended'].to(device)\n",
    "        transmission = batch['transmission'].to(device)\n",
    "        reflection = batch['reflection'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_trans, pred_ref = model(blended, is_train=model.training)\n",
    "        \n",
    "        loss = reflection_removal_loss(pred_trans, pred_ref, transmission, reflection, blended, perceptual_module)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            blended = batch['blended'].to(device)\n",
    "            transmission = batch['transmission'].to(device)\n",
    "            reflection = batch['reflection'].to(device)\n",
    "            \n",
    "            pred_trans, pred_ref = model(blended)\n",
    "            \n",
    "            loss = reflection_removal_loss(pred_trans, pred_ref, transmission, reflection, blended, perceptual_module)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model and visualizes results with a two-plot style:\n",
    "    1. Ground Truth Plot: Input, GT Transmission, GT Reflection.\n",
    "    2. Prediction Plot: Predicted Transmission, Inferred Reflection (with PSNR/SSIM).\n",
    "    \"\"\"\n",
    "    psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure().to(device)\n",
    "    \n",
    "    def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        tensor = tensor.clone()  # Avoid modifying original\n",
    "        for t, m, s in zip(tensor, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"Calculating metrics over the entire validation set.\")\n",
    "    with torch.no_grad():\n",
    "        # This part for calculating average metrics\n",
    "        for data in tqdm(dataloader, desc=\"Calculating Metrics\"):\n",
    "            if not data: continue\n",
    "            blended = data['blended'].to(device)\n",
    "            ground_truth_transmission = data['transmission'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                recon_bg, inferred_reflection = model(blended)  # Two outputs\n",
    "\n",
    "            h, w = ground_truth_transmission.shape[-2:]\n",
    "            recon_bg_resized = TF.resize(recon_bg, size=[h, w])\n",
    "            inferred_reflection_resized = TF.resize(inferred_reflection, size=[h, w])  # Resize inferred reflection\n",
    "            \n",
    "            psnr_metric.update(recon_bg_resized, ground_truth_transmission)\n",
    "            ssim_metric.update(recon_bg_resized, ground_truth_transmission)\n",
    "\n",
    "    avg_psnr = psnr_metric.compute()\n",
    "    avg_ssim = ssim_metric.compute()\n",
    "    \n",
    "    print(f\"\\nAverage PSNR: {avg_psnr:.4f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "    print(\"\\nDisplaying a few examples with the new plot style...\")\n",
    "    psnr_metric.reset() \n",
    "    ssim_metric.reset()\n",
    "    \n",
    "    images_shown = 0\n",
    "    for data in dataloader:\n",
    "        if images_shown >= 3:\n",
    "            break\n",
    "        if not data: continue\n",
    "\n",
    "        blended_vis, t_vis, r_vis = data['blended'].to(device), data['transmission'].to(device), data['reflection'].to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            recon_bg, inferred_reflection = model(blended_vis)  # Two outputs\n",
    "\n",
    "        h, w = t_vis.shape[-2:]\n",
    "        recon_bg_resized = TF.resize(recon_bg, size=[h, w])\n",
    "        inferred_reflection_resized = TF.resize(inferred_reflection, size=[h, w])\n",
    "\n",
    "        image_psnr = psnr_metric(recon_bg_resized, t_vis)\n",
    "        image_ssim = ssim_metric(recon_bg_resized, t_vis)\n",
    "\n",
    "        gt_images = {\n",
    "            \"Input Image\": denormalize(blended_vis.cpu().detach().squeeze(0)),\n",
    "            \"Ground Truth Transmission\": t_vis.cpu().squeeze(0),\n",
    "            \"Ground Truth Reflection\": r_vis.cpu().detach().squeeze(0)\n",
    "        }\n",
    "        fig1, axs1 = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig1.suptitle(\"Ground Truth Comparison\", fontsize=16)\n",
    "        for ax, (title, img) in zip(axs1, gt_images.items()):\n",
    "            img_np = img.permute(1, 2, 0).numpy().astype(np.float32)\n",
    "            ax.imshow(np.clip(img_np, 0, 1))\n",
    "            ax.set_title(title)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        pred_images = {\n",
    "            \"Predicted Transmission\": recon_bg_resized.cpu().detach().squeeze(0),\n",
    "            \"Inferred Reflection\": inferred_reflection_resized.cpu().detach().squeeze(0)\n",
    "        }\n",
    "        fig2, axs2 = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        fig2.suptitle(f\"Model Predictions (Transmission PSNR: {image_psnr:.2f} dB, SSIM: {image_ssim:.4f})\", fontsize=16)\n",
    "        for ax, (title, img) in zip(axs2, pred_images.items()):\n",
    "            img_np = img.permute(1, 2, 0).numpy().astype(np.float32)\n",
    "            ax.imshow(np.clip(img_np, 0, 1))\n",
    "            ax.set_title(title)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        images_shown += 1\n",
    "\n",
    "print(\"\\nEvaluating the model on the validation set.\")\n",
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildSceneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset class for the SIRR Wildscene test set.\n",
    "    It walks through numbered subdirectories and loads m.jpg and g.jpg.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, crop_size=(224, 224)):\n",
    "        self.root_dir = root_dir\n",
    "        self.crop_size = crop_size\n",
    "        self.image_pairs = []\n",
    "        # Normalization for ImageNet pretrained models\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        # Walk through the root directory to find all scene folders\n",
    "        for image_pair_dir in sorted(os.listdir(root_dir), key=int):\n",
    "            image_pair_path = os.path.join(root_dir, image_pair_dir)\n",
    "            if os.path.isdir(image_pair_path):\n",
    "                mixed_path = os.path.join(image_pair_path, 'm.jpg')\n",
    "                gt_path = os.path.join(image_pair_path, 'g.jpg')\n",
    "                reflection_path = os.path.join(image_pair_path, 'r.jpg')\n",
    "               \n",
    "                # Ensure both files exist before adding them to the list\n",
    "                if os.path.exists(mixed_path) and os.path.exists(gt_path):\n",
    "                    self.image_pairs.append({\n",
    "                        'blended': mixed_path,\n",
    "                        'transmission': gt_path,\n",
    "                        'reflection': reflection_path\n",
    "                    })\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair_info = self.image_pairs[idx]\n",
    "       \n",
    "        blended_img = Image.open(pair_info['blended']).convert('RGB')\n",
    "        transmission_img = Image.open(pair_info['transmission']).convert('RGB')\n",
    "        reflection_img = Image.open(pair_info['reflection']).convert('RGB')\n",
    "\n",
    "        blended_img = TF.resize(blended_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "        transmission_img = TF.resize(transmission_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "        reflection_img = TF.resize(reflection_img, self.crop_size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "       \n",
    "        blended_tensor = self.normalize(TF.to_tensor(blended_img))\n",
    "        transmission_tensor = TF.to_tensor(transmission_img)\n",
    "        reflection_tensor = TF.to_tensor(reflection_img)\n",
    "           \n",
    "        return {\n",
    "            'blended': blended_tensor,\n",
    "            'transmission': transmission_tensor,\n",
    "            'reflection': reflection_tensor\n",
    "        }\n",
    "\n",
    "WILD_SCENE_PATH = \"./Wildscene/Wildscene\"\n",
    "wildscene_dataset = WildSceneDataset(root_dir=WILD_SCENE_PATH)\n",
    "test_loader = DataLoader(wildscene_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "evaluate_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "rapids-25.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
